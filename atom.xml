<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ShaoFOng&#39;s Blog</title>
  <subtitle>Believe Oneself</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-08-17T17:16:43.633Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>余少锋</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hive架构</title>
    <link href="http://yoursite.com/2017/08/18/Hive%E6%9E%B6%E6%9E%84/"/>
    <id>http://yoursite.com/2017/08/18/Hive架构/</id>
    <published>2017-08-17T16:43:39.000Z</published>
    <updated>2017-08-17T17:16:43.633Z</updated>
    
    <content type="html"><![CDATA[<p>　　Hive是一个基于Hadoop的数据仓库，最初由Facebook提供，使用HQL作为查询接口、HDFS作为存储底层、mapReduce作为执行层，设计目的是让SQL技能良好，但Java技能较弱的分析师可以查询海量数据，2008年facebook把Hive项目贡献给Apache。Hive提供了比较完整的SQL功能（本质是将SQL转换为MapReduce），自身最大的缺点就是执行速度慢。Hive有自身的元数据结构描述，可以使用MySql\ProstgreSql\oracle 等关系型数据库来进行存储，但请注意Hive中的所有数据都存储在HDFS中。<br><img src="/images/hive架构.png" alt="此处输入图片的描述"></p>
<p> Hive的体系结构可以分为以下几部分：<br>    （1）用户接口主要有三个：CLI，Client 和 WUI。其中最常用的是CLI，Cli启动的时候，会同时启动一个Hive副本。Client是Hive的客户端，用户连接至Hive Server。在启动 Client模式的时候，需要指出Hive Server所在节点，并且在该节点启动Hive Server。 WUI是通过浏览器访问Hive。<br>    （2）Hive将元数据存储在数据库中，如MySQL、derby。Hive中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。<br>    （3）解释器、编译器、优化器完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS中，并在随后有MapReduce调用执行。<br>    （4）Hive的数据存储在HDFS中，大部分的查询、计算由MapReduce完成（包含<em>的查询，比如select </em> from tbl不会生成MapRedcue任务）。<br>    Hive将元数据存储在RDBMS中，有三种模式可以连接到数据库：<br>    <strong>（1） 单用户模式</strong><br>    此模式连接到一个In-memory 的数据库Derby，一般用于Unit Test。<br>    <img src="/images/hive单用户模式.JPG" alt="此处输入图片的描述"></p>
<p><strong>（2）多用户模式</strong><br>通过网络连接到一个数据库中，是最经常使用到的模式。<br><img src="/images/hive多用户模式.JPG" alt="此处输入图片的描述"></p>
<p><strong>（3） 远程服务器模式</strong><br>用于非Java客户端访问元数据库，在服务器端启动MetaStoreServer，客户端利用Thrift协议通过MetaStoreServer访问元数据库。<br><img src="/images/hive远程服务器模式.JPG" alt="此处输入图片的描述"></p>
<p>对于数据存储，Hive没有专门的数据存储格式，也没有为数据建立索引，用户可以非常自由的组织Hive中的表，只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符，Hive就可以解析数据。Hive中所有的数据都存储在HDFS中，存储结构主要包括数据库、文件、表和视图。Hive中包含以下数据模型：Table内部表，External Table外部表，Partition分区，Bucket桶。Hive默认可以直接加载文本文件，还支持sequence file 、RCFile。<br>    Hive的数据模型介绍如下：<br>    <strong>（1）Hive数据库</strong><br>    类似传统数据库的DataBase，在第三方数据库里实际是一张表。简单示例命令行 hive &gt; create database test_database;<br>    <strong>（2）内部表</strong><br>    Hive的内部表与数据库中的Table在概念上是类似。每一个Table在Hive中都有一个相应的目录存储数据。例如一个表pvs，它在HDFS中的路径为/wh/pvs，其中wh是在hive-site.xml中由${hive.metastore.warehouse.dir} 指定的数据仓库的目录，所有的Table数据（不包括External Table）都保存在这个目录中。删除表时，元数据与数据都会被删除。<br>    内部表简单示例：<br>    创建数据文件：test_inner_table.txt<br>    创建表：create table test_inner_table (key string)<br>    加载数据：LOAD DATA LOCAL INPATH ‘filepath’ INTO TABLE test_inner_table<br>    查看数据：select <em> from test_inner_table;  select count(</em>) from test_inner_table<br>    删除表：drop table test_inner_table<br>    <strong>（3）外部表</strong><br>    外部表指向已经在HDFS中存在的数据，可以创建Partition。它和内部表在元数据的组织上是相同的，而实际数据的存储则有较大的差异。内部表的创建过程和数据加载过程这两个过程可以分别独立完成，也可以在同一个语句中完成，在加载数据的过程中，实际数据会被移动到数据仓库目录中；之后对数据对访问将会直接在数据仓库目录中完成。删除表时，表中的数据和元数据将会被同时删除。而外部表只有一个过程，加载数据和创建表同时完成（CREATE EXTERNAL TABLE ……LOCATION），实际数据是存储在LOCATION后面指定的 HDFS 路径中，并不会移动到数据仓库目录中。当删除一个External Table时，仅删除该链接。<br>    外部表简单示例：<br>    创建数据文件：test_external_table.txt<br>    创建表：create external table test_external_table (key string)<br>    加载数据：LOAD DATA INPATH ‘filepath’ INTO TABLE test_inner_table<br>    查看数据：select <em> from test_external_table;  •select count(</em>) from test_external_table<br>    删除表：drop table test_external_table<br>    <strong>（4）分区</strong><br>    Partition对应于数据库中的Partition列的密集索引，但是Hive中Partition的组织方式和数据库中的很不相同。在Hive中，表中的一个Partition对应于表下的一个目录，所有的Partition的数据都存储在对应的目录中。例如pvs表中包含ds和city两个Partition，则对应于ds = 20090801, ctry = US 的HDFS子目录为/wh/pvs/ds=20090801/ctry=US；对应于 ds = 20090801, ctry = CA 的HDFS子目录为/wh/pvs/ds=20090801/ctry=CA。<br>    分区表简单示例：<br>    创建数据文件：test_partition_table.txt<br>    创建表：create table test_partition_table (key string) partitioned by (dt string)<br>    加载数据：LOAD DATA INPATH ‘filepath’ INTO TABLE test_partition_table partition (dt=‘2006’)<br>    查看数据：select <em> from test_partition_table;  select count(</em>) from test_partition_table<br>    删除表：drop table test_partition_table<br>    <strong>（5）桶</strong><br>    Buckets是将表的列通过Hash算法进一步分解成不同的文件存储。它对指定列计算hash，根据hash值切分数据，目的是为了并行，每一个Bucket对应一个文件。例如将user列分散至32个bucket，首先对user列的值计算hash，对应hash值为0的HDFS目录为/wh/pvs/ds=20090801/ctry=US/part-00000；hash值为20的HDFS目录为/wh/pvs/ds=20090801/ctry=US/part-00020。如果想应用很多的Map任务这样是不错的选择。<br>    桶的简单示例：<br>    创建数据文件：test_bucket_table.txt<br>    创建表：create table test_bucket_table (key string) clustered by (key) into 20 buckets<br>    加载数据：LOAD DATA INPATH ‘filepath’ INTO TABLE test_bucket_table<br>    查看数据：select <em> from test_bucket_table;  set hive.enforce.bucketing = true;<br>    <strong>（6）Hive的视图</strong><br>    视图与传统数据库的视图类似。视图是只读的，它基于的基本表，如果改变，数据增加不会影响视图的呈现；如果删除，会出现问题。•如果不指定视图的列，会根据select语句后的生成。<br>    示例：create view test_view as select </em> from test</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;　　Hive是一个基于Hadoop的数据仓库，最初由Facebook提供，使用HQL作为查询接口、HDFS作为存储底层、mapReduce作为执行层，设计目的是让SQL技能良好，但Java技能较弱的分析师可以查询海量数据，2008年facebook把Hive项目贡献给Apa
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>对MapReduce的认识2</title>
    <link href="http://yoursite.com/2017/08/16/%E5%AF%B9MapReduce%E7%9A%84%E8%AE%A4%E8%AF%862/"/>
    <id>http://yoursite.com/2017/08/16/对MapReduce的认识2/</id>
    <published>2017-08-15T16:26:06.000Z</published>
    <updated>2017-08-15T16:26:33.672Z</updated>
    
    <content type="html"><![CDATA[<h2 id="了解input-split"><a href="#了解input-split" class="headerlink" title="了解input split"></a>了解input split</h2><p><strong>输入分片（input split）：</strong><br>在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务<br>输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组，输入分片（input split）往往和hdfs的block（块）关系很密切<br>假如我们设定hdfs的块的大小是64mb，如果我们输入有三个文件，大小分别是3mb、65mb和127mb，那么mapreduce会把3mb文件分为一个输入分片（input split），65mb则是两个输入分片（input split）而127mb也是两个输入分片（input split）<br>    即我们如果在map计算前做输入分片调整，例如合并小文件，那么就会有5个map任务将执行，而且每个map执行的数据大小不均，这个也是mapreduce优化计算的一个关键点。</p>
<h2 id="处理阶段map-gt-partitioner-combiner-shuffle-gt-reduce"><a href="#处理阶段map-gt-partitioner-combiner-shuffle-gt-reduce" class="headerlink" title="处理阶段map-&gt; [partitioner,combiner,shuffle]-&gt;reduce"></a>处理阶段map-&gt; [partitioner,combiner,shuffle]-&gt;reduce</h2><p><strong>1、conbiner：</strong> MapReduce的一种优化手段<br>每一个map都可能会产生大量的本地输出，Combiner的作用就是对map端的输出先做一次合并，以减少在map和reduce节点之间的数据传输量，以提高网络IO性能<br><strong>Combiner的作用：</strong><br>（1）Combiner实现本地key的聚合，对map输出的key排序value进行迭代<br>       如下所示：<br>　　     map: (K1, V1) → list(K2, V2) 　　combine: (K2, list(V2)) → list(K2, V2) 　　reduce: (K2, list(V2)) → list(K3, V3)</p>
<p>（2）Combiner还有本地reduce功能（其本质上就是一个reduce）<br>         例如wordcount的例子和找出value的最大值的程序<br>          combiner和reduce完全一致，如下所示：<br>　　          map: (K1, V1) → list(K2, V2) 　　     combine: (K2, list(V2)) → list(K3, V3) 　　     reduce: (K3, list(V3)) → list(K4, V4)<br>使用combiner之后，先完成的map会在本地聚合，提升速度。对于hadoop自带的wordcount的例子，value就是一个叠加的数字，所以map一结束就可以进行reduce的value叠加，而不必要等到所有的map结束再去进行reduce的value叠加。<br><img src="/images/combiner.png" alt="此处输入图片的描述"></p>
<p><strong>2、partitioner：</strong><br><img src="/images/partitioner.png" alt="此处输入图片的描述"><br>哪个key到哪个Reducer的分配过程，是由Partitioner规定的。</p>
<p>分区Partitioner主要作用在于以下两点<br> 1、根据业务需要，产生多个输出文件<br> 2、多个reduce任务并发运行，提高整体job的运行效率</p>
<p><strong>3、shuffle:</strong><br><img src="/images/shuffle.png" alt="此处输入图片的描述"><br><strong>Shuffle是什么</strong><br>针对多个map任务的输出按照不同的分区（Partition）通过网络复制到不同的reduce任务节点上，这个过程就称作为Shuffle。<br><img src="/images/shuffle过程.png" alt="此处输入图片的描述"></p>
<p><img src="/images/map端.png" alt="此处输入图片的描述"><br>1.在map端首先是InputSplit，在InputSplit中含有DataNode中的数据，每一个InputSplit都会分配一个Mapper任务，Mapper任务结束后产生<k2,v2>的输出，这些输出先存放在缓存中，每个map有一个环形内存缓冲区，用于存储任务的输出。默认大小100MB（io.sort.mb属性），一旦达到阀值0.8(io.sort.spil l.percent)，一个后台线程就把内容写到(spill)Linux本地磁盘中的指定目录（mapred.local.dir）下的新建的一个溢出写文件。<br>2.写磁盘前，要进行partition、sort和combine等操作。通过分区，将不同类型的数据分开处理，之后对不同分区的数据进行排序，如果有Combiner，还要对排序后的数据进行combine。等最后记录写完，将全部溢出文件合并为一个分区且排序的文件<br>3.最后将磁盘中的数据送到Reduce中，图中Map输出有三个分区，有一个分区数据被送到图示的Reduce任务中，剩下的两个分区被送到其他Reducer任务中。而图示的Reducer任务的其他的三个输入则来自其他节点的Map输出。<br><img src="/images/reduce端.png" alt="此处输入图片的描述"></k2,v2></p>
<ol>
<li>Copy阶段：Reducer通过Http方式得到输出文件的分区。<br>　　reduce端可能从n个map的结果中获取数据，而这些map的执行速度不尽相同，当其中一个map运行结束时，reduce就会从JobTracker中获取该信息。map运行结束后TaskTracker会得到消息，进而将消息汇报给JobTracker，reduce定时从JobTracker获取该信息，reduce端默认有5个数据复制线程从map端复制数据<br>2.Merge阶段：如果形成多个磁盘文件会进行合并<br>　　从map端复制来的数据首先写到reduce端的缓存中，同样缓存占用到达一定阈值后会将数据写到磁盘中，同样会进行partition、combine、排序等过程。如果形成了多个磁盘文件还会进行合并，最后一次合并的结果作为reduce的输入而不是写入到磁盘中<br>3.Reducer的参数：最后将合并后的结果作为输入传入Reduce任务中</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;了解input-split&quot;&gt;&lt;a href=&quot;#了解input-split&quot; class=&quot;headerlink&quot; title=&quot;了解input split&quot;&gt;&lt;/a&gt;了解input split&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;输入分片（input split）：
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>对MapReduce的认识</title>
    <link href="http://yoursite.com/2017/08/14/%E5%AF%B9MapReduce%E7%9A%84%E8%AE%A4%E8%AF%86/"/>
    <id>http://yoursite.com/2017/08/14/对MapReduce的认识/</id>
    <published>2017-08-14T03:34:12.000Z</published>
    <updated>2017-08-14T16:22:14.900Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介mr架构，4个对象"><a href="#简介mr架构，4个对象" class="headerlink" title="简介mr架构，4个对象"></a>简介mr架构，4个对象</h2><p>MapReduce的思想是“分而治之”。<br>1）Mapper负责“分”<br>把复杂的任务分解为若干个“简单的任务”来处理。“简单的任务”包含三层含义：<br>数据或计算的规模相对原任务要大大缩小<br>就近计算原则，任务会分配到存放着所需数据的节点上进行计算<br>这些小任务可以并行计算彼此间几乎没有依赖关系</p>
<p>2）Reducer负责对map阶段的结果进行汇总。<br>至于需要多少个Reducer，可以根据具体问题，<br>通过在mapred-site.xml配置文件里设置参数mapred.reduce.tasks的值，缺省值为1。</p>
<p><img src="/images/MapReduce工作机制.png" alt="此处输入图片的描述"></p>
<h1 id="Mapreduce作业的4个对象"><a href="#Mapreduce作业的4个对象" class="headerlink" title="Mapreduce作业的4个对象"></a>Mapreduce作业的4个对象</h1><p>1、<br>客户端（client）：编写mapreduce程序，配置作业，提交作业，这就是程序员完成的工作；</p>
<p>JobTracker：初始化作业，分配作业，与TaskTracker通信，协调整个作业的执行；</p>
<p>TaskTracker：保持与JobTracker的通信，在分配的数据片段上执行Map或Reduce任务，TaskTracker和JobTracker的不同有个很重要的方面，就是在执行任务时候TaskTracker可以有n多个，JobTracker则只会有一个（JobTracker只能有一个就和hdfs里namenode一样存在单点故障，我会在后面的mapreduce的相关问题里讲到这个问题的）<br>2、JobTracker：初始化作业，分配作业，与TaskTracker通信，协调整个作业的执行；<br>3、TaskTracker：保持与JobTracker的通信，在分配的数据片段上执行Map或Reduce任务，TaskTracker和JobTracker的不同有个很重要的方面，就是在执行任务时候TaskTracker可以有n多个，JobTracker则只会有一个（JobTracker只能有一个就和hdfs里namenode一样存在单点故障，我会在后面的mapreduce的相关问题里讲到这个问题的）<br>4、Hdfs：保存作业的数据、配置信息等等，最后的结果也是保存在hdfs上面</p>
<h2 id="mr工作流程"><a href="#mr工作流程" class="headerlink" title="mr工作流程"></a>mr工作流程</h2><p><img src="/images/MapReduce工作流程.png" alt="此处输入图片的描述"></p>
<p>按照时间顺序包括：<br>输入分片（input split）、<br>map阶段、<br>combiner阶段、<br>shuffle阶段和<br>reduce阶段。</p>
<p><strong>输入分片（input split）：</strong><br>在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务<br>输入分片（input split）存储的并非数据本身，而是一个分片长度和一个记录数据的位置的数组，输入分片（input split）往往和hdfs的block（块）关系很密切<br>假如我们设定hdfs的块的大小是64mb，如果我们输入有三个文件，大小分别是3mb、65mb和127mb，那么mapreduce会把3mb文件分为一个输入分片（input split），65mb则是两个输入分片（inputsplit）而127mb也是两个输入分片（input split）即我们如果在map计算前做输入分片调整，例如合并小文件，那么就会有5个map任务将执行，而且每个map执行的数据大小不均，这个也是mapreduce优化计算的一个关键点。</p>
<p><strong>map阶段：</strong><br>程序员编写好的map函数了，因此map函数效率相对好控制，而且一般map操作都是本地化操作也就是在数据存储节点上进行；</p>
<p><strong>combiner阶段：</strong><br>combiner阶段是程序员可以选择的，combiner其实也是一种reduce操作，因此我们看见WordCount类里是用reduce进行加载的。</p>
<p>Combiner是一个本地化的reduce操作，它是map运算的后续操作，主要是在map计算出中间文件前做一个简单的合并重复key值的操作，例如我们对文件里的单词频率做统计，map计算时候如果碰到一个hadoop的单词就会记录为1，但是这篇文章里hadoop可能会出现n多次，那么map输出文件冗余就会很多，因此在reduce计算前对相同的key做一个合并操作，那么文件会变小，这样就提高了宽带的传输效率，毕竟hadoop计算力宽带资源往往是计算的瓶颈也是最为宝贵的资源，但是combiner操作是有风险的，使用它的原则是combiner的输入不会影响到reduce计算的最终输入，<br>例如：如果计算只是求总数，最大值，最小值可以使用combiner，但是做平均值计算使用combiner的话，最终的reduce计算结果就会出错。</p>
<p><strong>shuffle阶段：</strong><br>将map的输出作为reduce的输入的过程就是shuffle了</p>
<p><strong>reduce阶段：</strong><br>和map函数一样也是程序员编写的，最终结果是存储在hdfs上的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;简介mr架构，4个对象&quot;&gt;&lt;a href=&quot;#简介mr架构，4个对象&quot; class=&quot;headerlink&quot; title=&quot;简介mr架构，4个对象&quot;&gt;&lt;/a&gt;简介mr架构，4个对象&lt;/h2&gt;&lt;p&gt;MapReduce的思想是“分而治之”。&lt;br&gt;1）Mapper负责
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop之hdfs</title>
    <link href="http://yoursite.com/2017/08/10/Hadoop%E4%B9%8Bhdfs/"/>
    <id>http://yoursite.com/2017/08/10/Hadoop之hdfs/</id>
    <published>2017-08-10T15:12:10.000Z</published>
    <updated>2017-08-10T16:03:55.224Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、简介hdfs架构"><a href="#一、简介hdfs架构" class="headerlink" title="一、简介hdfs架构"></a>一、简介hdfs架构</h2><h2 id="HDFS简单介绍"><a href="#HDFS简单介绍" class="headerlink" title="HDFS简单介绍"></a>HDFS简单介绍</h2><p>HDFS全称是Hadoop Distribute File System,是一个能运行在普通商用硬件上的分布式文件系统。<br>与其他分布式文件系统显著不同的特点是：</p>
<p>HDFS是一个高容错系统且能运行在各种低成本硬件上；<br>提供高吞吐量，适合于存储大数据集；<br>HDFS提供流式数据访问机制。<br>HDFS起源于Apache Nutch，现在是Apache Hadoop项目的核心子项目。</p>
<h2 id="HDFS设计假设和目标"><a href="#HDFS设计假设和目标" class="headerlink" title="HDFS设计假设和目标"></a>HDFS设计假设和目标</h2><p>硬件错误是常态<br>在数据中心，硬件异常应被视作常态而非异常态。<br>在一个大数据环境下，hdfs集群有大量物理机器构成，每台机器由很多硬件组成，整个因为某一个组件出错而出错的几率是很高的，<br>因此HDFS架构的一个核心设计目标就是能够快速检测硬件失效并快速从失效中恢复工作。<br>流式访问要求<br>在HDFS集群上运行的应用要求流式访问数据，HDFS设计为适用于批处理而非交互式处理，因此在架构设计时更加强调高吞吐量而非低延迟。<br>对于POSIX的标准访问机制比如随机访问会严重降低吞吐量，HDFS将忽略此机制。<br>大数据集<br>假定HDFS的典型文件大小是GB甚至TB大小的，HDFS设计重点是支持大文件，支持通过机器数量扩展以支持更大的集群，<br>单个集群应提供海量文件数量支持<br>简单一致性模型<br>HDFS提供的访问模型是一次写入多次读取的模型。写入后文件保持原样不动简化了数据一致性模型并且对应用来说，它能得到更高的吞吐量。<br>文件追加也支持。<br>移动计算比移动数据代价更低<br>HDFS利用了计算机系统的数据本地化原理，认为数据离CPU越近，性能更高。<br>HDFS提供接口让应用感知数据的物理存储位置。<br>异构软硬件平台兼容<br>HDFS被设计成能方便的从一个平台迁移到另外一个平台</p>
<h2 id="HDFS适用场景"><a href="#HDFS适用场景" class="headerlink" title="HDFS适用场景"></a>HDFS适用场景</h2><p>综合上述的设计假设和后面的架构分析，HDFS特别适合于以下场景：</p>
<p>顺序访问<br>比如提供流媒体服务等大文件存储场景<br>大文件全量访问<br>如要求对海量数据进行全量访问，OLAP等<br>整体预算有限<br>想利用分布式计算的便利，又没有足够的预算购买HPC、高性能小型机等场景<br>在如下场景其性能不尽如人意：</p>
<p>低延迟数据访问<br>低延迟数据访问意味着快速数据定位，比如10ms级别响应，系统若忙于响应此类要求，<br>则有悖于快速返回大量数据的假设。</p>
<p>大量小文件<br>大量小文件将占用大量的文件块会造成较大的浪费以及对元数据（namenode）是个严峻的挑战<br>多用户并发写入<br>并发写入违背数据一致性模型，数据可能不一致。<br>实时更新<br>HDFS支持append，实时更新会降低数据吞吐以及增加维护数据一致的代价。</p>
<h2 id="HDFS架构"><a href="#HDFS架构" class="headerlink" title="HDFS架构"></a>HDFS架构</h2><p>下面这张HDFS架构图来自于hadoop官方网站.<br><img src="/images/hdfs.png" alt="此处输入图片的描述"></p>
<p>从这上面可以看出，HDFS采取主从式C/S架构，HDFS的节点分为两种角色：<br>NameNode<br>NameNode提供文件元数据，访问日志等属性的存储、操作功能。<br>文件的基础信息等存放在NameNode当中，采用集中式存储方案。<br>DataNode<br>DataNode提供文件内容的存储、操作功能。<br>文件数据块本身存储在不同的DataNode当中，DataNode可以分布在不同机架。</p>
<p>HDFS的Client会分别访问NameNode和DataNode以获取文件的元信息以及内容。HDFS集群的Client将<br>直接访问NameNode和DataNode，相关数据直接从NameNode或者DataNode传送到客户端。</p>
<h2 id="二、hdfs读写策略"><a href="#二、hdfs读写策略" class="headerlink" title="二、hdfs读写策略"></a>二、hdfs读写策略</h2><h2 id="HDFS读文件过程："><a href="#HDFS读文件过程：" class="headerlink" title="HDFS读文件过程："></a>HDFS读文件过程：</h2><p><img src="/images/hdfsRead.png" alt="此处输入图片的描述"></p>
<p>客户端通过调用FileSystem对象的open()来读取希望打开的文件。对于HDFS来说，这个对象是分布式文件系统的一个实例。<br>DistributedFileSystem通过RPC来调用namenode，以确定文件的开头部分的块位置。对于每一块，namenode返回具有该块副本的datanode地址。此外，这些datanode根据他们与client的距离来排序（根据网络集群的拓扑）。如果该client本身就是一个datanode，便从本地datanode中读取。DistributedFileSystem 返回一个FSDataInputStream对象给client读取数据，FSDataInputStream转而包装了一个DFSInputStream对象。<br>接着client对这个输入流调用read()。存储着文件开头部分块的数据节点地址的DFSInputStream随即与这些块最近的datanode相连接。<br>通过在数据流中反复调用read()，数据会从datanode返回client。<br>到达块的末端时，DFSInputStream会关闭与datanode间的联系，然后为下一个块找到最佳的datanode。client端只需要读取一个连续的流，这些对于client来说都是透明的。<br>在读取的时候，如果client与datanode通信时遇到一个错误，那么它就会去尝试对这个块来说下一个最近的块。它也会记住那个故障节点的datanode，以保证不会再对之后的块进行徒劳无益的尝试。client也会确认datanode发来的数据的校验和。如果发现一个损坏的块，它就会在client试图从别的datanode中读取一个块的副本之前报告给namenode。<br>这个设计的一个重点是，client直接联系datanode去检索数据，并被namenode指引到块中最好的datanode。因为数据流在此集群中是在所有datanode分散进行的。所以这种设计能使HDFS可扩展到最大的并发client数量。同时，namenode只不过提供块的位置请求（存储在内存中，十分高效），不是提供数据。否则如果客户端数量增长，namenode就会快速成为一个“瓶颈”。</p>
<h2 id="HDFS写文件过程"><a href="#HDFS写文件过程" class="headerlink" title="HDFS写文件过程:"></a>HDFS写文件过程:</h2><p><img src="/images/hdfsWrite.png" alt="此处输入图片的描述"></p>
<p>客户端通过在DistributedFileSystem中调用create()来创建文件。<br>DistributedFileSystem 使用RPC去调用namenode，在文件系统的命名空间创一个新的文件，没有块与之相联系。namenode执行各种不同的检查以确保这个文件不会已经存在，并且在client有可以创建文件的适当的许可。如果检查通过，namenode就会生成一个新的文件记录；否则，文件创建失败并向client抛出一个IOException异常。分布式文件系统返回一个文件系统数据输出流，让client开始写入数据。就像读取事件一样，文件系统数据输出流控制一个DFSOutputStream，负责处理datanode和namenode之间的通信。<br>在client写入数据时，DFSOutputStream将它分成一个个的包，写入内部队列，称为数据队列。数据流处理数据队列，数据流的责任是根据适合的datanode的列表要求namenode分配适合的新块来存储数据副本。这一组datanode列表形成一个管线————假设副本数是3，所以有3个节点在管线中。<br>数据流将包分流给管线中第一个的datanode，这个节点会存储包并且发送给管线中的第二个datanode。同样地，第二个datanode存储包并且传给管线中的第三个数据节点。<br>DFSOutputStream也有一个内部的数据包队列来等待datanode收到确认，称为确认队列。一个包只有在被管线中所有的节点确认后才会被移除出确认队列。如果在有数据写入期间，datanode发生故障， 则会执行下面的操作，当然这对写入数据的client而言是透明的。首先管线被关闭，确认队列中的任何包都会被添加回数据队列的前面，以确保故障节点下游的datanode不会漏掉任意一个包。为存储在另一正常datanode的当前数据块制定一个新的标识，并将该标识传给namenode，以便故障节点datanode在恢复后可以删除存储的部分数据块。从管线中删除故障数据节点并且把余下的数据块写入管线中的两个正常的datanode。namenode注意到块复本量不足时，会在另一个节点上创建一个新的复本。后续的数据块继续正常接收处理。只要dfs.replication.min的副本（默认是1）被写入，写操作就是成功的，并且这个块会在集群中被异步复制，直到其满足目标副本数（dfs.replication 默认值为3)。<br>client完成数据的写入后，就会在流中调用close()。<br>在向namenode节点发送完消息之前，此方法会将余下的所有包放入datanode管线并等待确认。namenode节点已经知道文件由哪些块组成（通过Data streamer 询问块分配），所以它只需在返回成功前等待块进行最小量的复制。<br>复本的布局：需要对可靠性、写入带宽和读取带宽进行权衡。Hadoop的默认布局策略是在运行客户端的节点上放第1个复本（如果客户端运行在集群之外，就随机选择一个节点，不过系统会避免挑选那些存储太满或太忙的节点。）第2个复本放在与第1个复本不同且随机另外选择的机架的节点上（离架）。第3个复本与第2个复本放在相同的机架，且随机选择另一个节点。其他复本放在集群中随机的节点上，不过系统会尽量避免相同的机架放太多复本。<br>总的来说，这一方法不仅提供了很好的稳定性（数据块存储在两个机架中）并实现很好的负载均衡，包括写入带宽（写入操作只需要遍历一个交换机）、读取性能（可以从两个机架中选择读取）和集群中块的均匀分布（客户端只在本地机架上写入一个块）。</p>
<p>参考文档：<br>1、<a href="http://blog.csdn.net/norriszhang/article/details/39697005" target="_blank" rel="external">http://blog.csdn.net/norriszhang/article/details/39697005</a><br>2、参考自《Hadoop权威指南》 [<a href="http://www.cnblogs.com/swanspouse/p/5137308.html" target="_blank" rel="external">http://www.cnblogs.com/swanspouse/p/5137308.html</a>]</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、简介hdfs架构&quot;&gt;&lt;a href=&quot;#一、简介hdfs架构&quot; class=&quot;headerlink&quot; title=&quot;一、简介hdfs架构&quot;&gt;&lt;/a&gt;一、简介hdfs架构&lt;/h2&gt;&lt;h2 id=&quot;HDFS简单介绍&quot;&gt;&lt;a href=&quot;#HDFS简单介绍&quot; cla
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>大数据的简单认识</title>
    <link href="http://yoursite.com/2017/08/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E7%AE%80%E5%8D%95%E8%AE%A4%E8%AF%86/"/>
    <id>http://yoursite.com/2017/08/08/大数据的简单认识/</id>
    <published>2017-08-07T16:43:22.000Z</published>
    <updated>2017-08-07T17:03:07.617Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、什么是大数据"><a href="#一、什么是大数据" class="headerlink" title="一、什么是大数据"></a>一、什么是大数据</h2><p>　　“大数据”是一个体量特别大，数据类别特别大的数据集，并且这样的数据集无法用传统数据库工具对其内容进行抓取、管理和处理。 “大数据”首先是指数据体量(volumes)?大，指代大型数据集，一般在10TB?规模左右，但在实际应用中，很多企业用户把多个数据集放在一起，已经形成了PB级的数据量；其次是指数据类别(variety)大，数据来自多种数据源，数据种类和格式日渐丰富，已冲破了以前所限定的结构化数据范畴，囊括了半结构化和非结构化数据。接着是数据处理速度（Velocity）快，在数据量非常庞大的情况下，也能够做到数据的实时处理。最后一个特点是指数据真实性（Veracity）高，随着社交数据、企业内容、交易与应用数据等新数据源的兴趣，传统数据源的局限被打破，企业愈发需要有效的信息之力以确保其真实性及安全性。　　“大数据”是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。从数据的类别上看，”大数据”指的是无法使用传统流程或工具处理或分析的信息。它定义了那些超出正常处理范围和大小、迫使用户采用非传统处理方法的数据集。<br>　　亚马逊网络服务（AWS）、大数据科学家JohnRauser提到一个简单的定义：大数据就是任何超过了一台计算机处理能力的庞大数据量。<br>　　研发小组对大数据的定义：”大数据是最大的宣传技术、是最时髦的技术，当这种现象出现时，定义就变得很混乱。” Kelly说：”大数据是可能不包含所有的信息，但我觉得大部分是正确的。对大数据的一部分认知在于，它是如此之大，分析它需要多个工作负载，这是AWS的定义。当你的技术达到极限时，也就是数据的极限”。 大数据不是关于如何定义，最重要的是如何使用。最大的挑战在于哪些技术能更好的使用数据以及大数据的应用情况如何。这与传统的数据库相比，开源的大数据分析工具的如Hadoop的崛起，这些非结构化的数据服务的价值在哪里。</p>
<h2 id="二、数据-结构-非结构"><a href="#二、数据-结构-非结构" class="headerlink" title="二、数据 结构+非结构"></a>二、数据 结构+非结构</h2><p>　　在信息社会，信息可以划分为两大类。一类信息能够用数据或统一的结构加以表示，我们称之为结构化数据，如数字、符号；而另一类信息无法用数字或统一的结构表示，如文本、图像、声音、网页等，我们称之为非结构化数据。结构化数据属于非结构化数据，是非结构化数据的特例。 </p>
<p>定义：</p>
<p>　　结构化数据：即行数据,存储在数据库里,可以用二维表结构来逻辑表达实现的数据。</p>
<p>　　非结构化数据：包括所有格式的办公文档、文本、图片、XML、HTML、各类报表、图像和音频/视频信息等等。</p>
<p>　　半结构化数据：就是介于完全结构化数据（如关系型数据库、面向对象数据库中的数据）和完全无结构的数据（如声音、图像文件等）之间的数据，HTML文档就属于半结构化数据。它一般是自描述的，数据的结构和内容混在一起，没有明显的区分。</p>
<p>示例：</p>
<p>　数据模型：<br>　　结构化数据：二维表（关系型）<br>　　半结构化数据：树、图<br>　　非结构化数据：无<br>　　RMDBS的数据模型有：如网状数据模型、层次数据模型、关系型<br>　　其他：<br>　　结构化数据：先有结构、再有数据<br>　　半结构化数据：先有数据，再有结构</p>
<h2 id="三、数据单位"><a href="#三、数据单位" class="headerlink" title="三、数据单位"></a>三、数据单位</h2><p>存储单位:<br>计算机存储单位一般用B,KB、MB、GB、TB、PB、EB、ZB、YB、BB来表示,它们之间的关系是：<br>位 bit (比特)(Binary Digits)：存放一位二进制数,即 0 或 1,最小的存储单位.<br>字节 byte：8个二进制位为一个字节(B),最常用的单位.<br>1KB (Kilobyte 千字节)=1024B,<br>1MB (Megabyte 兆字节 简称“兆”)=1024KB,<br>1GB (Gigabyte 吉字节 又称“千兆”)=1024MB,<br>1TB (Trillionbyte 万亿字节 太字节)=1024GB,其中1024=2^10 ( 2 的10次方),<br>1PB（Petabyte 千万亿字节 拍字节）=1024TB,<br>1EB（Exabyte 百亿亿字节 艾字节）=1024PB,<br>1ZB (Zettabyte 十万亿亿字节 泽字节)= 1024 EB,<br>1YB (Jottabyte 一亿亿亿字节 尧字节)= 1024 ZB,<br>1BB (Brontobyte 一千亿亿亿字节)= 1024 YB.</p>
<h2 id="四、-工作流程-采集-预处理ETL-分析-显示"><a href="#四、-工作流程-采集-预处理ETL-分析-显示" class="headerlink" title="四、 工作流程 采集-预处理ETL-分析-显示"></a>四、 工作流程 采集-预处理ETL-分析-显示</h2><p>大数据技术之数据采集ETL：<br>       这里不过多的说数据采集的过程，可以简单的理解：有数据库就会有数据。</p>
<pre><code>这里我们更关注数据的ETL过程，而ETL前期的过程，只需要了解其基本范畴就OK。

在数据挖掘的范畴了，数据清洗的前期过程，可简单的认为就是ETL的过程。ETL的发展过程伴随着数据挖掘至今，其相关技术也已非常成熟。这里我们也不过多的探讨ETL过程，日后如有涉及，在细分。
</code></pre><p>概念：<br>       ETL（extract提取、transform转换、load加载）。ETL负责将分散的、异构数据源中的数据如关系数据、平面数据文件等抽取到临时中间层后，进行清洗、转换、集成，最后加载到数据仓库或数据集市中，成为联机分析处理、数据挖掘提供决策支持的数据。</p>
<pre><code>ETL是构建数据仓库的重要的一环，用户从数据源抽取所需的数据，经过数据清洗，最终按照预先定义好的数据仓库模型，将数据加载到数据仓库中。其定义域来源也不下于十几年，技术发展也应相当成熟。可乍眼一看，似乎并没有什么技术可言，也没有什么深奥之处，但在实际的项目中，却常常在这个环节上耗费太多的人力，而在后期的维护上，往往更费脑筋。导致上面的原因，往往是在项目初期没有正确的估计ETL的工作，没有认真的考虑其与工具支撑有很大的关系。

   在做ETL产品选型的时候，任然必不可少的要面临四点（成本、人员经验、案例和技术支持）来考量。在做ETL的过程中，也随之产生于一些ETL工具，如Datastage、Powercenter、ETLAutomation。而在实际ETL工具应用的对比上，对元数据的支持、对数据质量的支持、维护的方便性、定制开发功能的支持等方面是我们选择的切入点。一个项目，从数据源到最终目标表，多则达上百个ETL过程，少则也十几个。这些过程之间的依赖关系、出错控制以及恢复的流程处理，都是工具需要重点考虑。这里不再多讨论，具体应用再具体说明。
</code></pre><h2 id="什么是数据预处理？"><a href="#什么是数据预处理？" class="headerlink" title="什么是数据预处理？"></a>什么是数据预处理？</h2><p>数据预处理指的是如下过程：</p>
<p>采集原始数据</p>
<p>理解原始数据</p>
<p>清洗原始数据</p>
<p>为余下的数据分析或建模做准备</p>
<h2 id="五、计算模式"><a href="#五、计算模式" class="headerlink" title="五、计算模式"></a>五、计算模式</h2><p>1.大数据查询分析计算模式与典型系统</p>
<p>由于行业数据规模的增长已大大超过了传统的关系数据库的承载和处理能力，因此，目前需要尽快研究并提供面向大数据存储管理和查询分析的新的技术方法和系统，尤其要解决在数据体量极大时如何能够提供实时或准实时的数据查询分析能力，满足企业日常的管理需求。然而，大数据的查询分析处理具有很大的技术挑战，在数量规模较大时，即使采用分布式数据存储管理和并行化计算方法，仍然难以达到关系数据库处理中小规模数据时那样的秒级响应性能。</p>
<p>大数据查询分析计算的典型系统包括Hadoop下的HBase和Hive、Facebook公司开发的Cassandra、Google公司的Dremel、Cloudera公司的实时查询引擎Impala；此外为了实现更高性能的数据查询分析，还出现了不少基于内存的分布式数据存储管理和查询系统，如Apache Spark下的数据仓库Shark、SAP公司的Hana、开源的Redis等。</p>
<p>2.批处理计算模式与典型系统</p>
<p>最适合于完成大数据批处理的计算模式是MapReduce，这是MapReduce设计之初的主要任务和目标。MapReduce是一个单输入、两阶段（Map和Reduce）的数据处理过程。首先，MapReduce对具有简单数据关系、易于划分的大规模数据采用“分而治之”的并行处理思想；然后将大量重复的数据记录处理过程总结成Map和Reduce两个抽象的操作；最后MapReduce提供了一个统一的并行计算框架，把并行计算所涉及到的诸多系统层细节都交给计算框架去完成，以此大大简化了程序员进行并行化程序设计的负担。</p>
<p>MapReduce的简单易用性使其成为目前大数据处理最成功的主流并行计算模式。在开源社区的努力下，开源的Hadoop系统目前已成为较为成熟的大数据处理平台，并已发展成一个包括众多数据处理工具和环境的完整的生态系统。目前几乎国内外的各个著名IT企业都在使用Hadoop平台进行企业内大数据的计算处理。此外，Spark系统也具备批处理计算的能力。</p>
<p>3.流式计算模式与典型系统</p>
<p>流式计算是一种高实时性的计算模式，需要对一定时间窗口内应用系统产生的新数据完成实时的计算处理，避免造成数据堆积和丢失。很多行业的大数据应用，如电信、电力、道路监控等行业应用以及互联网行业的访问日志处理，都同时具有高流量的流式数据和大量积累的历史数据，因而在提供批处理计算模式的同时，系统还需要能具备高实时性的流式计算能力。流式计算的一个特点是数据运动、运算不动，不同的运算节点常常绑定在不同的服务器上。</p>
<p>Facebook的Scribe和Apache的Flume都提供了一定的机制来构建日志数据处理流图。而更为通用的流式计算系统是Twitter公司的Storm、Yahoo公司的S4以及Apache Spark Steaming。</p>
<p>4.迭代计算模式与典型系统</p>
<p>为了克服Hadoop MapReduce难以支持迭代计算的缺陷，工业界和学术界对Hadoop MapReduce进行了不少改进研究。HaLoop把迭代控制放到MapReduce作业执行的框架内部，并通过循环敏感的调度器保证前次迭代的Reduce输出和本次迭代的Map输入数据在同一台物理机上，以减少迭代间的数据传输开销；iMapReduce在这个基础上保持Map和Reduce任务的持久性，规避启动和调度开销；而Twister在前两者的基础上进一步引入了可缓存的Map和Reduce对象，利用内存计算和pub/sub网络进行跨节点数据传输。</p>
<p>目前，一个具有快速和灵活的迭代计算能力的典型系统是Spark，其采用了基于内存的RDD数据集模型实现快速的迭代计算。</p>
<p>5.图计算模式与典型系统</p>
<p>社交网络、Web链接关系图等都包含大量具有复杂关系的图数据，这些图数据规模很大，常常达到数十亿的顶点和上万亿的边数。这样大的数据规模和非常复杂的数据关系，给图数据的存储管理和计算分析带来了很大的技术难题。用MapReduce计算模式处理这种具有复杂数据关系的图数据通常不能适应，为此，需要引入图计算模式。</p>
<p>大规模图数据处理首先要解决数据的存储管理问题，通常大规模图数据也需要使用分布式存储方式。但是，由于图数据具有很强的数据关系，分布式存储就带来了一个重要的图划分问题（Graph Partitioning）。根据图数据问题本身的特点，图划分可以使用“边切分”和“顶点切分”两种方式。在有效的图划分策略下，大规模图数据得以分布存储在不同节点上，并在每个节点上对本地子图进行并行化处理。与任务并行和数据并行的概念类似，由于图数据并行处理的特殊性，人们提出了一个新的“图并行”（Graph Parallel）的概念。事实上，图并行是数据并行的一个特殊形式，需要针对图数据处理的特征考虑一些特殊的数据组织模型和计算方法。</p>
<p>目前已经出现了很多分布式图计算系统，其中较为典型的系统包括Google公司的Pregel、Facebook对Pregel的开源实现Giraph、微软公司的Trinity、Spark下的GraphX，以及CMU的GraphLab以及由其衍生出来的目前性能最快的图数据处理系统PowerGraph。</p>
<p>6.内存计算模式与典型系统</p>
<p>Hadoop MapReduce为大数据处理提供了一个很好的平台。然而，由于MapReduce设计之初是为大数据线下批处理而设计的，随着数据规模的不断扩大，对于很多需要高响应性能的大数据查询分析计算问题，现有的以Hadoop为代表的大数据处理平台在计算性能上往往难以满足要求。随着内存价格的不断下降以及服务器可配置的内存容量的不断提高，用内存计算完成高速的大数据处理已经成为大数据计算的一个重要发展趋势。例如，Hana系统设计者总结了很多实际的商业应用后发现，一个提供50TB总内存容量的计算集群将能够满足绝大多数现有的商业系统对大数据的查询分析处理要求，如果一个服务器节点可配置1TB～2TB的内存，则需要25～50个服务器节点。目前Intel Xeon E-7系列处理器最大可支持高达1.5TB的内存，因此，配置一个上述大小规模的内存计算集群是可以做到的。</p>
<h2 id="六、分布式系统"><a href="#六、分布式系统" class="headerlink" title="六、分布式系统"></a>六、分布式系统</h2><p>在《分布式系统概念与设计》一书中，对分布式系统做了如下定义：</p>
<p>分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统<br>简单来说就是一群独立计算机集合共同对外提供服务，但是对于系统的用户来说，就像是一台计算机在提供服务一样。分布式意味着可以采用更多的普通计算机（相对于昂贵的大型机）组成分布式集群对外提供服务。计算机越多，CPU、内存、存储资源等也就越多，能够处理的并发访问量也就越大。</p>
<p>从分布式系统的概念中我们知道，各个主机之间通信和协调主要通过网络进行，所以，分布式系统中的计算机在空间上几乎没有任何限制，这些计算机可能被放在不同的机柜上，也可能被部署在不同的机房中，还可能在不同的城市中，对于大型的网站甚至可能分布在不同的国家和地区。但是，无论空间上如何分布，一个标准的分布式系统应该具有以下几个主要特征：</p>
<p>分布性</p>
<p>分布式系统中的多台计算机之间在空间位置上可以随意分布，系统中的多台计算机之间没有主、从之分，即没有控制整个系统的主机，也没有受控的从机。<br>透明性</p>
<p>系统资源被所有计算机共享。每台计算机的用户不仅可以使用本机的资源，还可以使用本分布式系统中其他计算机的资源(包括CPU、文件、打印机等)。<br>同一性</p>
<p>系统中的若干台计算机可以互相协作来完成一个共同的任务，或者说一个程序可以分布在几台计算机上并行地运行。<br>通信性</p>
<p>系统中任意两台计算机都可以通过通信来交换信息。<br>和集中式系统相比，分布式系统的性价比更高、处理能力更强、可靠性更高、也有很好的扩展性。但是，分布式在解决了网站的高并发问题的同时也带来了一些其他问题。首先，分布式的必要条件就是网络，这可能对性能甚至服务能力造成一定的影响。其次，一个集群中的服务器数量越多，服务器宕机的概率也就越大。另外，由于服务在集群中分布是部署，用户的请求只会落到其中一台机器上，所以，一旦处理不好就很容易产生数据一致性问题。</p>
<h2 id="七、CDH"><a href="#七、CDH" class="headerlink" title="七、CDH"></a>七、CDH</h2><p>CDH简介　</p>
<p>　　• Cloudera’s Distribution, including Apache Hadoop<br>　　• 是Hadoop众多分支中的一种，由Cloudera维护，基于稳定版本的Apache Hadoop构建<br>　　• 提供了Hadoop的核心<br>　　　　– 可扩展存储<br>　　　　– 分布式计算<br>　　• 基于Web的用户界面<br>　　<br>CDH的优点　　</p>
<p>　　• 版本划分清晰<br>　　• 版本更新速度快<br>　　• 支持Kerberos安全认证<br>　　• 文档清晰<br>　　• 支持多种安装方式（Cloudera Manager方式）</p>
<h2 id="八、CAP原理"><a href="#八、CAP原理" class="headerlink" title="八、CAP原理"></a>八、CAP原理</h2><p>分布式领域CAP理论，<br>Consistency(一致性), 数据一致更新，所有数据变动都是同步的<br>Availability(可用性), 好的响应性能<br>Partition tolerance(分区容错性) 可靠性</p>
<p>C（一致性）：所有的节点上的数据时刻保持同步<br>A（可用性）：每个请求都能接受到一个响应，无论响应成功或失败<br>P（分区容错）：系统应该能持续提供服务，即使系统内部有消息丢失（分区）</p>
<h2 id="九、Hadoop技术栈"><a href="#九、Hadoop技术栈" class="headerlink" title="九、Hadoop技术栈"></a>九、Hadoop技术栈</h2><ol>
<li>hadoop 生态概况</li>
</ol>
<p>Hadoop是一个由Apache基金会所开发的分布式系统基础架构。</p>
<p>用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。</p>
<p>具有可靠、高效、可伸缩的特点。</p>
<p>Hadoop的核心是YARN,HDFS和Mapreduce</p>
<p>下图是hadoop生态系统，集成spark生态圈。在未来一段时间内，hadoop将于spark共存，hadoop与spark</p>
<p>都能部署在yarn、mesos的资源管理系统之上<br>2、HDFS（Hadoop分布式文件系统）</p>
<p>源自于Google的GFS论文，发表于2003年10月，HDFS是GFS克隆版。</p>
<p>HDFS是Hadoop体系中数据存储管理的基础。它是一个高度容错的系统，能检测和应对硬件故障，用于在低成本的通用硬件上运行。</p>
<p>HDFS简化了文件的一致性模型，通过流式数据访问，提供高吞吐量应用程序数据访问功能，适合带有大型数据集的应用程序。</p>
<p>它提供了一次写入多次读取的机制，数据以块的形式，同时分布在集群不同物理机器上。</p>
<p>3、Mapreduce（分布式计算框架）</p>
<p>源自于google的MapReduce论文，发表于2004年12月，Hadoop MapReduce是google MapReduce 克隆版。</p>
<p>MapReduce是一种分布式计算模型，用以进行大数据量的计算。它屏蔽了分布式计算框架细节，将计算抽象成map和reduce两部分，</p>
<p>其中Map对数据集上的独立元素进行指定的操作，生成键-值对形式中间结果。Reduce则对中间结果中相同“键”的所有“值”进行规约，以得到最终结果。</p>
<p>MapReduce非常适合在大量计算机组成的分布式并行环境里进行数据处理。</p>
<ol>
<li>HBASE（分布式列存数据库）</li>
</ol>
<p>源自Google的Bigtable论文，发表于2006年11月，HBase是Google Bigtable克隆版</p>
<p>HBase是一个建立在HDFS之上，面向列的针对结构化数据的可伸缩、高可靠、高性能、分布式和面向列的动态模式数据库。</p>
<p>HBase采用了BigTable的数据模型：增强的稀疏排序映射表（Key/Value），其中，键由行关键字、列关键字和时间戳构成。</p>
<p>HBase提供了对大规模数据的随机、实时读写访问，同时，HBase中保存的数据可以使用MapReduce来处理，它将数据存储和并行计算完美地结合在一起。</p>
<ol>
<li>Zookeeper（分布式协作服务）</li>
</ol>
<p>源自Google的Chubby论文，发表于2006年11月，Zookeeper是Chubby克隆版</p>
<p>解决分布式环境下的数据管理问题：统一命名，状态同步，集群管理，配置同步等。</p>
<p>Hadoop的许多组件依赖于Zookeeper，它运行在计算机集群上面，用于管理Hadoop操作。</p>
<ol>
<li>HIVE（数据仓库）</li>
</ol>
<p>由facebook开源，最初用于解决海量结构化的日志数据统计问题。</p>
<p>Hive定义了一种类似SQL的查询语言(HQL),将SQL转化为MapReduce任务在Hadoop上执行。通常用于离线分析。</p>
<p>HQL用于运行存储在Hadoop上的查询语句，Hive让不熟悉MapReduce开发人员也能编写数据查询语句，然后这些语句被翻译为Hadoop上面的MapReduce任务。</p>
<p>7.Pig(ad-hoc脚本）</p>
<p>由yahoo!开源，设计动机是提供一种基于MapReduce的ad-hoc(计算在query时发生)数据分析工具</p>
<p>Pig定义了一种数据流语言—Pig Latin，它是MapReduce编程的复杂性的抽象,Pig平台包括运行环境和用于分析Hadoop数据集的脚本语言(Pig Latin)。</p>
<p>其编译器将Pig Latin翻译成MapReduce程序序列将脚本转换为MapReduce任务在Hadoop上执行。通常用于进行离线分析。</p>
<p>8.Sqoop(数据ETL/同步工具）</p>
<p>Sqoop是SQL-to-Hadoop的缩写，主要用于传统数据库和Hadoop之前传输数据。数据的导入和导出本质上是Mapreduce程序，充分利用了MR的并行化和容错性。</p>
<p>Sqoop利用数据库技术描述数据架构，用于在关系数据库、数据仓库和Hadoop之间转移数据。</p>
<p>参考博客：<br>1、<a href="http://www.cnblogs.com/raphael5200/p/5293960.html" target="_blank" rel="external">http://www.cnblogs.com/raphael5200/p/5293960.html</a><br>2、<a href="http://blog.jobbole.com/95588/" target="_blank" rel="external">http://blog.jobbole.com/95588/</a><br>3、<a href="http://book.51cto.com/art/201408/449311.htm" target="_blank" rel="external">http://book.51cto.com/art/201408/449311.htm</a><br>4、<a href="http://blog.csdn.net/sky1203850702/article/details/47977071" target="_blank" rel="external">http://blog.csdn.net/sky1203850702/article/details/47977071</a><br>5、<a href="http://blog.csdn.net/kingmax54212008/article/details/46237417" target="_blank" rel="external">http://blog.csdn.net/kingmax54212008/article/details/46237417</a><br>6、<a href="http://blog.csdn.net/utnewbear/article/details/8267653" target="_blank" rel="external">http://blog.csdn.net/utnewbear/article/details/8267653</a><br>7、<a href="https://www.zhihu.com/question/23896161" target="_blank" rel="external">https://www.zhihu.com/question/23896161</a><br>8、<a href="http://www.360doc.com/content/16/0407/11/15491230_548541129.shtml" target="_blank" rel="external">http://www.360doc.com/content/16/0407/11/15491230_548541129.shtml</a><br>9、<a href="http://www.cnblogs.com/gridmix/p/5102694.html" target="_blank" rel="external">http://www.cnblogs.com/gridmix/p/5102694.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、什么是大数据&quot;&gt;&lt;a href=&quot;#一、什么是大数据&quot; class=&quot;headerlink&quot; title=&quot;一、什么是大数据&quot;&gt;&lt;/a&gt;一、什么是大数据&lt;/h2&gt;&lt;p&gt;　　“大数据”是一个体量特别大，数据类别特别大的数据集，并且这样的数据集无法用传统数据库工具
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>相对布局，绝对布局的简单解读（position:relative与position:absolute）</title>
    <link href="http://yoursite.com/2017/04/05/%E7%9B%B8%E5%AF%B9%E5%B8%83%E5%B1%80%EF%BC%8C%E7%BB%9D%E5%AF%B9%E5%B8%83%E5%B1%80%E7%9A%84%E7%AE%80%E5%8D%95%E8%A7%A3%E8%AF%BB/"/>
    <id>http://yoursite.com/2017/04/05/相对布局，绝对布局的简单解读/</id>
    <published>2017-04-05T06:36:37.000Z</published>
    <updated>2017-08-01T06:46:07.392Z</updated>
    
    <content type="html"><![CDATA[<p>W3C是这样定义的：<br>-relative:生成相对定位的元素，相对于其正常位置进行定位。<br>因此，”left:20” 会向元素的 LEFT 位置添加 20 像素。</p>
<p>-absolute:生成绝对定位的元素，相对于 static 定位以外的第一个父元素进行定位。<br>元素的位置通过 “left”, “top”, “right” 以及 “bottom” 属性进行规定。</p>
<p>个人认为比较重要的是absolute会脱离文档流，而relative不会脱离文档流,怎么理解呢？<br>首先：<br>什么是文档流？</p>
<pre><code>将窗体自上而下分成一行行，并在每行中按从左至右的顺序排放元素，即为文档流。
</code></pre><p>举个例子：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">   //初始</div><div class="line">   <span class="tag">&lt;<span class="name">style</span> <span class="attr">type</span>=<span class="string">"text/css"</span>&gt;</span><span class="undefined"></span></div><div class="line">	#first&#123;width:200px;height:100px;border:1px solid red;&#125;</div><div class="line">	#second&#123;width:200px;height:100px;border:1px solid blue;&#125;</div><div class="line"><span class="tag">&lt;/<span class="name">style</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></div><div class="line">   	<span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"first"</span>&gt;</span>first<span class="tag">&lt;/<span class="name">div</span>&gt;</span></div><div class="line">   	<span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"second"</span>&gt;</span>second<span class="tag">&lt;/<span class="name">div</span>&gt;</span></div><div class="line">   <span class="tag">&lt;/<span class="name">body</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>初始图：<br><img src="/images/init.png" alt="此处输入图片的描述"><br><figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">   //修改position为relative并设top、left为20px</div><div class="line"><span class="tag">&lt;<span class="name">style</span> <span class="attr">type</span>=<span class="string">"text/css"</span>&gt;</span><span class="undefined"></span></div><div class="line">	#first&#123;position:relative;width:200px;height:100px;border:1px solid red;top:20px;left:20px;&#125;</div><div class="line">	#second&#123;width:200px;height:100px;border:1px solid blue;&#125;</div><div class="line"><span class="tag">&lt;/<span class="name">style</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>效果图：<br><img src="/images/relative.png" alt="此处输入图片的描述"><br>设置relative+margin后：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">   <span class="tag">&lt;<span class="name">style</span> <span class="attr">type</span>=<span class="string">"text/css"</span>&gt;</span><span class="undefined"></span></div><div class="line">	#first&#123;position:relative;width:200px;height:100px;border:1px solid red;top:20px;left:20px;margin:20px;&#125;</div><div class="line">	#second&#123;width:200px;height:100px;border:1px solid blue;&#125;</div><div class="line"><span class="tag">&lt;/<span class="name">style</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>效果图：<br><img src="/images/absolute.png" alt="此处输入图片的描述"></p>
<p>修改position为absolute的话<br><figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">   <span class="tag">&lt;<span class="name">style</span> <span class="attr">type</span>=<span class="string">"text/css"</span>&gt;</span><span class="undefined"></span></div><div class="line">	#first&#123;position:absolute;width:200px;height:100px;border:1px solid red;top:20px;left:20px;&#125;</div><div class="line">	#second&#123;width:200px;height:100px;border:1px solid blue;&#125;</div><div class="line"><span class="tag">&lt;/<span class="name">style</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>效果图：<br>![此处输入图片的描述][4]</p>
<p>参考文章：<a href="http://blog.csdn.net/chen_zw/article/details/8741365" target="_blank" rel="external">http://blog.csdn.net/chen_zw/article/details/8741365</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;W3C是这样定义的：&lt;br&gt;-relative:生成相对定位的元素，相对于其正常位置进行定位。&lt;br&gt;因此，”left:20” 会向元素的 LEFT 位置添加 20 像素。&lt;/p&gt;
&lt;p&gt;-absolute:生成绝对定位的元素，相对于 static 定位以外的第一个父元素进
    
    </summary>
    
      <category term="css" scheme="http://yoursite.com/categories/css/"/>
    
    
      <category term="css" scheme="http://yoursite.com/tags/css/"/>
    
  </entry>
  
  <entry>
    <title>get跟post的区别</title>
    <link href="http://yoursite.com/2017/03/21/get%E8%B7%9Fpost%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://yoursite.com/2017/03/21/get跟post的区别/</id>
    <published>2017-03-20T18:14:21.000Z</published>
    <updated>2017-07-11T08:40:50.913Z</updated>
    
    <content type="html"><![CDATA[<h2 id="首先我们应该简单了解一下什么是HTTP？"><a href="#首先我们应该简单了解一下什么是HTTP？" class="headerlink" title="首先我们应该简单了解一下什么是HTTP？"></a>首先我们应该简单了解一下什么是HTTP？</h2><pre><code>HTTP的全称是HyperText Transfer Protocol（超文本传输协议），超文本是一种带有链接的文本，而传

输协议是一种获取一些东西从一个地方到另一个地方的规则，这些规则是为了传输页面到你的浏览器。

http的工作方式：客户端（浏览器）向服务器提交HTTP请求，服务器向客户端返回响应。
</code></pre><hr>
<h2 id="HTTP请求的方法"><a href="#HTTP请求的方法" class="headerlink" title="HTTP请求的方法"></a>HTTP请求的方法</h2><pre><code>HTTP定义了与服务器交互的不同方法基本的有： GET、POST、PUT、DELETE。这个时候不得不提到URL这

个东西，URL全称是UniformResourceLocator，中文意思是统一资源定位符（资源描述符），一个URL用

于描述一个网络上的资源，HTTP中的GET、POST、PUT、DELETE这些方法就是对这个资源进行查、改、

增、删，不过我们常用的主要是GET跟POST（网上看到一个原因是传统的Web MVC框架基本上都只支持GET

和POST两种HTTP方法，而不支持PUT和DELETE方法。）

w3school对get跟post给出了一般的理解：
</code></pre><p>   <img src="/images/getandpost.png" alt="此处输入图片的描述"></p>
<hr>
<h2 id="为什么说POST比GET跟安全呢"><a href="#为什么说POST比GET跟安全呢" class="headerlink" title="为什么说POST比GET跟安全呢"></a>为什么说POST比GET跟安全呢</h2><pre><code>GET请求的数据会附在URL后面，以?分割URL跟传输的数据，参数之间以&amp;相连，比如:

login.action?name=xiaoming&amp;password=abcdefg

这样一来用户名，密码就会出现在URL中，别人要拿到你用户名密码只要通过浏览器的历史记录，就能轻松拿到

而POST是将传输的数据放在HTTP包的包体中进行传输。
</code></pre><hr>
<h2 id="为什么不放弃GET而用POST呢？"><a href="#为什么不放弃GET而用POST呢？" class="headerlink" title="为什么不放弃GET而用POST呢？"></a>为什么不放弃GET而用POST呢？</h2><pre><code>既然post有这么多好处，为什么我们不放弃get，只用post算了呢?这里需要考虑效率问题，

get传输数据比post快:
</code></pre><ol>
<li><p>POST请求包含了请求头<br>post需要在请求的body部分包含数据，所以会多了几个数据描述部分的首部字段，比如content-type</p>
<p>，不过这对于传输速度影响不大。</p>
</li>
<li><p>真正影响传输速度的是post发送数据之前会先将请求头发送给服务器确认，服务器确认之后，才发送数据。</p>
<p><strong>post请求的过程：</strong></p>
</li>
</ol>
<p>　　1.浏览器请求tcp连接（第一次握手）</p>
<p>　　2.服务器答应进行tcp连接（第二次握手）</p>
<p>　　3.浏览器确认，并发送post请求头（第三次握手，这个报文比较小，所以http会在此时进行第一次数据发送）</p>
<p>　　4.服务器返回100 continue响应</p>
<p>　　5.浏览器开始发送数据</p>
<p>　　6.服务器返回200 ok响应</p>
<p>　&nbsp;&nbsp;<strong>get请求的过程：</strong></p>
<p>　  &nbsp;1.浏览器请求tcp连接（第一次握手）</p>
<p>　　2.服务器答应进行tcp连接（第二次握手）</p>
<p>　　3.浏览器确认，并发送get请求头和数据（第三次握手，这个报文比较小，所以http会在此时进行第一次数据发送）</p>
<p>　　4.服务器返回200 ok响应<br>　　<br>　　<br>　　<br>　　<strong>哪里有错误或者需要补充的欢迎留言^.^</strong><br>　　<br>　　参考文章：
　　</p>
<ul>
<li>浅谈HTTP中Get与Post的区别   <a href="http://www.cnblogs.com/hyddd/archive/2009/03/31/1426026.html" target="_blank" rel="external">http://www.cnblogs.com/hyddd/archive/2009/03/31/1426026.html</a></li>
<li>W3CSCHOOL     HTTP 方法：GET 对比 POST  <a href="http://www.w3school.com.cn/tags/html_ref_httpmethods.asp" target="_blank" rel="external">http://www.w3school.com.cn/tags/html_ref_httpmethods.asp</a></li>
<li>迷途小哈  为什么get比post更快 <a href="http://www.cnblogs.com/strayling/p/3580048.html" target="_blank" rel="external">http://www.cnblogs.com/strayling/p/3580048.html</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;首先我们应该简单了解一下什么是HTTP？&quot;&gt;&lt;a href=&quot;#首先我们应该简单了解一下什么是HTTP？&quot; class=&quot;headerlink&quot; title=&quot;首先我们应该简单了解一下什么是HTTP？&quot;&gt;&lt;/a&gt;首先我们应该简单了解一下什么是HTTP？&lt;/h2&gt;&lt;
    
    </summary>
    
      <category term="技术" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="技术" scheme="http://yoursite.com/tags/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
</feed>
